1
00:00:00,729 --> 00:00:06,089
“We’re entering an era in which our enemies
can make anyone say anything at any point

2
00:00:06,089 --> 00:00:07,089
in time.”

3
00:00:07,089 --> 00:00:11,699
Jordan Peele created this fake video of President
Obama to demonstrate how easy it was to put

4
00:00:11,699 --> 00:00:13,309
words in someone else’s mouth-

5
00:00:13,309 --> 00:00:17,190
moving forward we need to be more vigilant
with what we trust from the internet.

6
00:00:17,190 --> 00:00:22,170
not everyone bought it, but the technology
behind it is rapidly improving, even as worries

7
00:00:22,170 --> 00:00:24,669
increase about its potential for harm.

8
00:00:24,669 --> 00:00:28,449
This is your Bloomberg QuickTake on Fake Videos.

9
00:00:28,449 --> 00:00:32,580
Deep fakes, or realistic-looking fake videos
and audio, gained popularity as a means of

10
00:00:32,580 --> 00:00:34,960
adding famous actresses into porn scenes.

11
00:00:34,960 --> 00:00:38,720
Despite bans on major websites, they remain
easy to make and find.

12
00:00:38,720 --> 00:00:43,760
They’re named for the deep-learning AI algorithms
that make them possible.

13
00:00:43,760 --> 00:00:48,739
Input real audio or video of a specific person-
the more, the better- and the software tries

14
00:00:48,739 --> 00:00:51,500
to recognize patterns in speech and movement.

15
00:00:51,500 --> 00:00:55,180
Introduce a new element like someone else’s
face or voice, and a deep fake is born.

16
00:00:55,180 --> 00:00:59,920
Jeremy Kahn: It's actually extremely easy
to make one of these things… there was just

17
00:00:59,920 --> 00:01:04,629
some breakthroughs from academic researchers
who work with this particular kind of machine

18
00:01:04,629 --> 00:01:08,850
learning in the past few weeks, which would
drastically reduce the amount of video you

19
00:01:08,850 --> 00:01:10,410
need actually to create one of these.

20
00:01:10,410 --> 00:01:15,860
Programs like FakeApp, the most popular one
for making deep fakes, need dozens of hours

21
00:01:15,860 --> 00:01:20,980
of human assistance to create a video that
looks like this rather than this, but that’s

22
00:01:20,980 --> 00:01:21,980
changing.

23
00:01:21,980 --> 00:01:24,930
In September researchers at Carnegie-Mellon
revealed unsupervised software that accurately

24
00:01:24,930 --> 00:01:33,750
reproduced not just facial features, but changing
weather patterns and flowers in bloom as well.

25
00:01:33,750 --> 00:01:36,570
But with increasing capability comes increasing
concern.

26
00:01:36,570 --> 00:01:39,270
You know, this is kind of fake news on steroids
potentially.

27
00:01:39,270 --> 00:01:45,360
We do not know of a case yet where someone
has tried to use this to perpetrate a kind

28
00:01:45,360 --> 00:01:52,650
of fraud or an information warfare campaign,
or for that matter, to really damage someone’s

29
00:01:52,650 --> 00:01:56,230
reputation// but it’s the danger that everyone
is really afraid of.

30
00:01:56,230 --> 00:02:01,860
In a world where fakes are easy to create-
authenticity also becomes easier to deny.

31
00:02:01,860 --> 00:02:05,320
People caught doing genuinely objectionable
things could claim evidence against them is

32
00:02:05,320 --> 00:02:06,590
bogus.

33
00:02:06,590 --> 00:02:11,910
Fake videos are also difficult to detect,
though researchers and the US Department of

34
00:02:11,910 --> 00:02:14,739
Defense, in particular, have said they’re
working on ways to counter them.

35
00:02:14,739 --> 00:02:20,159
Deep Fakes do however have some positive potential-
take CereProc, who creates digital voices

36
00:02:20,159 --> 00:02:26,849
for people who lose theirs from disease…

37
00:02:26,849 --> 00:02:31,900
There are also applications that could be
considered more value-neutral, like the many,

38
00:02:31,900 --> 00:02:37,400
many deep fakes that exist solely to turn
as many movies as possible into Nicolas Cage

39
00:02:37,400 --> 00:00:00,000
movies.

