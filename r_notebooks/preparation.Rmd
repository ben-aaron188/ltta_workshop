---
title: "Preparation and loading the data"
subtitle: "EuroCSS workshop: LTTA"
output: html_notebook
author:  B Kleinberg https://github.com/ben-aaron188
---

### Welcome to the EuroCSS workshop on Linguistic Temporal Trajectory Analysis

In order to save time at the day of the workshop, we'd kindly ask you to set up your R workspace, test the functionality of your workspace with a few scripts, and download the data we will use for the paper hackathon.

This script will guide you through the necessary steps.


### Step 1: Getting the workshop data

Use one of the two options to get access to the data.

#### Option 1: forking (and cloning) the GitHub repo

You can access the data by [forking](https://help.github.com/articles/fork-a-repo/) the workshop GitHub repo available on this URL: [https://github.com/ben-aaron188/ltta_workshop](https://github.com/ben-aaron188/ltta_workshop). You can then clone it to your machine so you have it available and can work with full version control on it.

#### Option 2: downloading the repo as a .zip file

You can also simply download the repository as a .zip file using the green 'Clone or download' button on the upper right part of the repository [here](https://github.com/ben-aaron188/ltta_workshop).


Once you have completed either option 1 or 2, you should have a folder called *ltta_workshop*. In that folder, you can find the general workshop information as well as a sub-directory called *"workshop_data"*. This folder is the main data folder for this workshop.

Navigate to that folder in R Studio.

```{r}
#E.g.
setwd('SOMETHING/ltta_workshop/workshop_data')
```


### Step 2: Installing/loading required R dependencies

#### R packages from CRAN

These packages are available on CRAN and can be installed within R Studio using the command below.

```{r}
#clean your R memory
rm(list = ls())

#stringr
if (!require(stringr)){
  install.packages('stringr')
} 
library(stringr)

#data.table
if (!require(data.table)){
  install.packages('data.table')
} 
library(data.table)

#lexicon
if (!require(lexicon)){
  install.packages('lexicon')
} 
library(lexicon)

#syuzhet
if (!require(syuzhet)){
  install.packages('syuzhet')
} 
library(syuzhet)
```


#### R dependencies from GitHub/source

The *naive context sentiment* code is "live" available on [GitHub](https://github.com/ben-aaron188/naive_context_sentiment). 
- If you want to contribute to that code, it is best to [clone this repository to your machine](https://help.github.com/articles/cloning-a-repository/).
- Alternatively, you can load the dependency from the main directory of the LTTA folder using this command:

```{r}
source('./r_deps/naive_context_sentiment/ncs.R')
#using this method ensures you have the NCS dependency. It does not update to any commits made in the future (use the GitHub dependency for this for future use)
```

The *txt_df_from_dir.R* function loads .txt files from a (nested) directory into an R dataframe. This makes the text data (e.g., transcripts) useable for later analysis. Code is "live" available on [GitHub](https://github.com/ben-aaron188/r_helper_functions/blob/master/txt_df_from_dir.R) and in the local source. 

```{r}
source('./r_deps/txt_df_from_dir.R')
```

### Step 3: Loading the workshop data into R

*Important*: The following steps assume that you are in the `./ltta_workshop/workshop_data` folder.

In that folder, we have a number of datasets:

- sample data of a YouTube vlogger adopted from [Kleinberg et al., 2018, EMNLP](https://arxiv.org/ftp/arxiv/papers/1808/1808.09722.pdf). We will use these data in the practical.
- data for the hackathon (details about that dataset will be outlined in the workshop)

The following steps guide you through laoding and testing these datasets.

#### Load the "YouTube vlogger" sample data

This dataset consists of all YouTube vlog transcripts from [Casey Neistat](https://www.youtube.com/user/caseyneistat) until April 2018.

This script will guide you through the full, minimal LTTA workflow.

1. Load raw YouTube transcript data from individual .txt files to an R dataframe

```{r}
vlogs_casey_neistat = txt_df_from_dir(dirpath = './sample_data/caseyneistat'
                                      , recursive = F
                                      , include_processed = F
                                      , to_lower = F)

View(head(vlogs_casey_neistat))
```

2. Extract dynamic sentiment shapes (= the core of LTTA on the intra-textual level)

```{r}
#this code extracts the sentiment shapes for each vlog transcript
#Note: this might take 1-2 minutes
sentiment_shapes = ncs_full(txt_input_col = vlogs_casey_neistat$text
                        , txt_id_col = vlogs_casey_neistat$id
                        , low_pass_filter_size = 5
                        , transform_values = T
                        , normalize_values = F
                        , min_tokens = 10 #minimum length required to process a vlog
                        , cluster_lower = 3 #window size before sentiment
                        , cluster_upper = 3 #window size after sentiment
                        )

View(sentiment_shapes[, 1:10])
#Note that each transcript is represented in one column with 100 rows (= standardized narrative time)

#set names to corresponding file name
names(sentiment_shapes) = vlogs_casey_neistat$Filename
```

3. Analyse & visualise the sentiment shapes

```{r}
#Show the sentiment shape of the vlog named '779.txt'

plot(sentiment_shapes$`779.txt`
     , type='h'
     , ylim = c(-1.25, 1.25)
     , main = 'Shape of file 770.txt'
     , ylab = 'Sentiment'
     , xlab = 'Standardized narrative time')
```

```{r}
#Some descriptives: proportion of sentiment above 0

prop.table(table(sentiment_shapes$`779.txt` > 0))
```

#### Load the "hackathon" data

```{r}
#load the RData file of the sampled data
load('./hackathon_data/main_data/eurocss_ltta_workshop_data_sampled.RData')

head(dt.sampled_balanced)
```


We have extracted some features from the transcripts already:
- unigrams
- bigrams
- trigrams
- LIWC variables (info [here](https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf))
- POS proportions (using the [qdap package](https://rdrr.io/cran/qdap/man/pos.html))
- sentiment shapes (as done above)

You can load these as follows:

```{r}
#load some features already extracted
##"static" features: ngrams
load('./hackathon_data/features/eurocss_ltta_workshop_data_unigrams.RData')
df.tfidf_ngrams_1[1:100, 100:120]

load('./hackathon_data/features/eurocss_ltta_workshop_data_bigrams.RData')
df.tfidf_ngrams_2[1:100, 100:120]

load('./hackathon_data/features/eurocss_ltta_workshop_data_trigrams.RData')
df.tfidf_ngrams_3[1:100, 100:120]

##LIWC
load('./hackathon_data/features/eurocss_ltta_workshop_data_LIWC_notext.RData')
dt.data_liwc_notext

##POS
load('./hackathon_data/features/eurocss_ltta_workshop_data_POS.RData')
dt.data_pos_notext

##sentiment shapes
load('./hackathon_data/features/eurocss_ltta_workshop_data_sentiment_notext.RData')
dt.data_sentiment_notext
```


-------------------

